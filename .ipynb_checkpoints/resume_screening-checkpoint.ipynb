{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk,re\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer,SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('chained_assignment',None) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(folder):\n",
    "    _all = os.listdir(folder)\n",
    "    _all = [folder+'\\\\' + i for i in _all]\n",
    "    return _all\n",
    "\n",
    "def extract_folder(folder):\n",
    "\ttext_list = []\n",
    "\tfor _file in get_all(folder):\n",
    "\t\tstri = open(_file, 'r',encoding=\"utf8\").read()\n",
    "\t\t# stri = \" \".join(stri.split()).translate(str.maketrans('', '', string.punctuation))\n",
    "\t\tstri = \" \".join(stri.split())\n",
    "\t\ttext_list.append(stri)\n",
    "\treturn text_list\n",
    "\n",
    "def reject_list():\n",
    "\treturn set(extract_folder('output')) - set(extract_folder('pass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def remove_numbers(words):\n",
    "\tno_numbers = []\n",
    "\tfor word in words:\n",
    "\t\tno_number = ''.join([i for i in word if not i.isdigit()])\n",
    "\t\tno_numbers.append(no_number)\n",
    "\treturn no_numbers\n",
    "\n",
    "def remove_emails(paragraph):\n",
    "\tregex = r\"\\S*@\\S*\\s?\"\n",
    "\tresult = re.sub(regex, \"\", paragraph, 0)\n",
    "\treturn result\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    # words = remove_emails(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    # words = replace_numbers(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = stem_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_train(vect,X_train):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print(('Features: ', X_train_dtm.shape[1]))\n",
    "    return X_train_dtm\n",
    "\n",
    "def transform(vect,X_test):\n",
    "\tX_test_dtm = vect.transform(X_test)\n",
    "\treturn X_test_dtm\n",
    "\n",
    "def train(X_train_dtm, y_train):\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    return nb\n",
    "\n",
    "def run_test(X_test_dtm):\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    # print(y_pred_class)\n",
    "    return y_pred_class\n",
    "\n",
    "def metric(y_test,y_pred_class):\n",
    "\tfrom sklearn import metrics\n",
    "\tprint('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))\n",
    "\tprint('Precision: ', metrics.precision_score(y_test, y_pred_class))\n",
    "\tprint('Recall: ', metrics.recall_score(y_test, y_pred_class))\n",
    "\n",
    "\n",
    "def predict(matrix):\n",
    "\tresult = nb.predict(matrix)\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def norminalize_all(pass_df):\n",
    "    for i in range(len(pass_df)):\n",
    "\t    a = remove_emails(pass_df[\"text\"][i])\n",
    "\t    a = nltk.WhitespaceTokenizer().tokenize(a)\n",
    "\t    c = ' '.join(map(str, a))\n",
    "\t    c = nltk.tokenize.WordPunctTokenizer().tokenize(c)\n",
    "\t    c = normalize(c)\n",
    "\t    c = ' '.join(map(str, c))\n",
    "\t    e = nltk.WhitespaceTokenizer().tokenize(c)\n",
    "\t    e = ' '.join(map(str, e))\n",
    "\t    pass_df[\"text\"][i] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_df = pd.DataFrame({\"text\":extract_folder('pass'),\"passs\":1})\n",
    "reject_df = pd.DataFrame({\"text\":extract_folder('reject'),\"passs\":0})\n",
    "pass_df = pass_df.append(reject_df, ignore_index=True)\n",
    "new_df = pd.DataFrame({\"text\":extract_folder('new-resume')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "norminalize_all(pass_df)\n",
    "norminalize_all(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ang run hao sebast singap cit pasir ris driv educ nanyang technolog univers bachel engin chem bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alis gao mobl email educ nat univers singap singap b sc maj stat spec fin busy stat hono distinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quek jian hong joel address minton hougang street singap e mail contact cur aug educ nat univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e mail tel jingyu zhang pres educ background nat univers singap maj msc fin engin cours stochast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>christopher tan educ imp colleg busy school london uk msc man recipy icb scholarship award relev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text\n",
       "0  ang run hao sebast singap cit pasir ris driv educ nanyang technolog univers bachel engin chem bi...\n",
       "1  alis gao mobl email educ nat univers singap singap b sc maj stat spec fin busy stat hono distinc...\n",
       "2  quek jian hong joel address minton hougang street singap e mail contact cur aug educ nat univers...\n",
       "3  e mail tel jingyu zhang pres educ background nat univers singap maj msc fin engin cours stochast...\n",
       "4  christopher tan educ imp colleg busy school london uk msc man recipy icb scholarship award relev..."
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['text']\n",
    "X = pass_df[feature_cols]\n",
    "y = pass_df['passs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=20, stratify =y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "kf = model_selection.StratifiedKFold(n_splits=4, shuffle=True ,random_state =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 2930)\n",
      "(16, 2930)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "X_new = new_df['text']\n",
    "print((X_train_dtm.shape))\n",
    "print((X_test_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast other feature columns to float and convert to a sparse matrix.\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2930)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine sparse matrices.\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2930)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat for testing set.\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.6875)\n",
      "('Precision: ', 0.6666666666666666)\n",
      "('Recall: ', 0.5714285714285714)\n",
      "[1 1 1 0 0 1 0 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression with text column only.\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "metric(y_test,y_pred_class)\n",
    "X_new_dtm = transform(vect,X_new)\n",
    "print(logreg.predict(X_new_dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.6875)\n",
      "('Precision: ', 0.6666666666666666)\n",
      "('Recall: ', 0.5714285714285714)\n",
      "[1 0 1 1 0 1 0 1 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Multinominal\n",
    "nb = MultinomialNB()\n",
    "nb = train(X_train_dtm, y_train)\n",
    "y_pred_class = run_test(X_test_dtm)\n",
    "metric(y_test,y_pred_class)\n",
    "X_new_dtm = transform(vect,X_new)\n",
    "print(predict(X_new_dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ CROSS VALIDATION each fold ~~~ CREDIT TO ANTON ~~~~\n",
      "Model 1\n",
      "ROC AUC: 0.8015873015873016\n",
      "('Accuracy: ', 0.8125)\n",
      "('Precision: ', 0.8333333333333334)\n",
      "('Recall: ', 0.7142857142857143)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Model 2\n",
      "ROC AUC: 0.6746031746031746\n",
      "('Accuracy: ', 0.6875)\n",
      "('Precision: ', 0.6666666666666666)\n",
      "('Recall: ', 0.5714285714285714)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Model 3\n",
      "ROC AUC: 0.875\n",
      "('Accuracy: ', 0.8666666666666667)\n",
      "('Precision: ', 0.7777777777777778)\n",
      "('Recall: ', 1.0)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Model 4\n",
      "ROC AUC: 0.7708333333333333\n",
      "('Accuracy: ', 0.7857142857142857)\n",
      "('Precision: ', 0.8)\n",
      "('Recall: ', 0.6666666666666666)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
      "Mean of Accuracy for all folds : 0.7880952380952381 \n",
      "Mean of ROC AUC: 0.7805059523809523\n",
      "[1 1 1 0 0 1 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "accuracy = []\n",
    "roc = []\n",
    "matrix = []\n",
    "\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~ CREDIT TO ANTON ~~~~\")\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    vect = CountVectorizer()\n",
    "    X_train_dtm = vect.fit_transform(X.iloc[train_index].text)\n",
    "    X_test_dtm = vect.transform(X.iloc[test_index].text)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train_dtm, y.iloc[train_index])\n",
    "    y_pred_class = logreg.predict(X_test_dtm)\n",
    "    X_new_dtm = transform(vect,X_new)\n",
    "    prediction = logreg.predict(X_new_dtm)\n",
    "    accuracy.append(np.mean(y.iloc[test_index] == logreg.predict(X_test_dtm)))\n",
    "    lr_pred_proba = logreg.predict_proba(X_test_dtm)[:,1]\n",
    "    matrix.append(metrics.confusion_matrix(y_true=y.iloc[test_index], y_pred=lr_pred_proba > .5))\n",
    "    roc.append(metrics.roc_auc_score(y_true=y.iloc[test_index], y_score=lr_pred_proba> .5))\n",
    "    n += 1\n",
    "    \n",
    "    print('Model {}'.format(n))\n",
    "    print('ROC AUC: {}'.format(roc[n-1]))\n",
    "    metric(y.iloc[test_index],y_pred_class)\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "\n",
    "print(\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print('Mean of Accuracy for all folds : {} '.format(np.mean(accuracy)))\n",
    "print('Mean of ROC AUC: {}'.format(np.mean(roc)))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
